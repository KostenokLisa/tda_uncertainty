{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QePQJVVC2Qcm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def prim_algo_simplified(adjacency_matrix):\n",
        "    n = len(adjacency_matrix)\n",
        "\n",
        "    infty = torch.max(adjacency_matrix).item() + 10\n",
        "    dst = torch.ones(n, device=adjacency_matrix.device) * infty\n",
        "    ancestors = -torch.ones(n, dtype=int, device=adjacency_matrix.device)\n",
        "    visited = torch.zeros(n, dtype=bool, device=adjacency_matrix.device)\n",
        "\n",
        "    s, v = 0, 0\n",
        "    for i in range(n - 1):\n",
        "        visited[v] = 1\n",
        "\n",
        "        ancestors[dst > adjacency_matrix[v]] = v\n",
        "        dst = torch.minimum(dst, adjacency_matrix[v])\n",
        "        dst[visited] = infty\n",
        "        v = torch.argmin(dst)\n",
        "\n",
        "        s += adjacency_matrix[v][ancestors[v]]\n",
        "\n",
        "    return s\n",
        "\n",
        "class RTD_simplified_summ_only(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def __call__(self, a1, a2):\n",
        "        r1 = (a1 / torch.quantile(a1, 0.9))\n",
        "        r2 = (a2 / torch.quantile(a2, 0.9))\n",
        "        rmin = torch.minimum(r1, r2)\n",
        "\n",
        "        rmin_sum = prim_algo_simplified(rmin)\n",
        "        r1_sum = prim_algo_simplified(r1)\n",
        "        r2_sum = prim_algo_simplified(r2)\n",
        "\n",
        "        return (r1_sum - rmin_sum + r2_sum - rmin_sum)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# head idx is fixed, layer idx varies from 0 to 12\n",
        "attention_12_head = np.load(\"/content/sample_data/attention_train_12_head.npy\")\n",
        "# layer idx is fixed, head idx varies from 0 to 12\n",
        "attention_12_layer = np.load(\"/content/sample_data/attention_train_12_layer.npy\")"
      ],
      "metadata": {
        "id": "VpXRLQSJ2f6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rtd_approx = RTD_simplified_summ_only()"
      ],
      "metadata": {
        "id": "qAgOI1AU2m24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = attention_12_head.shape[0]\n",
        "N_stats = 144 # 12 heads * 12 layers in attention \"cross\"\n",
        "device = \"cuda\"\n",
        "\n",
        "# for each pair of attention matrices count rtd scores using fast approximation\n",
        "cross_barcodes_stats = np.zeros((N, N_stats))\n",
        "for i in range(N):\n",
        "  print(\"Sample number:\", i)\n",
        "  for j in range(12):\n",
        "    for k in range(12):\n",
        "      a1 = (attention_12_layer[i, j, :, :]).astype(float)\n",
        "      a2 = (attention_12_head[i, k, :, :]).astype(float)\n",
        "      a1 = torch.tensor(a1).to(device)\n",
        "      a2 = torch.tensor(a2).to(device)\n",
        "      rtd_score = rtd_approx(a1, a2)\n",
        "      res[i, 12 * j + k] = rtd_score"
      ],
      "metadata": {
        "id": "_BWU5haw3kSK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}